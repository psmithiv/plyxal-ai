{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18937f2c-7489-4edb-8d4e-5544b17faaa1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# The variables that typically require your input and customization in the code are as follows:\n",
    "\n",
    "1. **Project Setup Variables (Step 1 and 2):**\n",
    "   - `project_name`: You should choose a unique name for your Scrapy project.\n",
    "   - `spider_name`: Name your Scrapy spider based on your project's purpose.\n",
    "   - `start_url`: Specify the starting URL for web scraping, relevant to your project.\n",
    "\n",
    "2. **Fine-Tuning BERT Variables (Step 6):**\n",
    "   - `X_train`: Input data for fine-tuning (text content). You need to provide the training data, which should be a list of text samples.\n",
    "   - `y_train`: Target labels for fine-tuning. You should provide the corresponding labels for your training data.\n",
    "   - `model_path`: Specify the path where you want to save the fine-tuned BERT model.\n",
    "   - `num_epochs`: Determine the number of training epochs based on your dataset and task.\n",
    "\n",
    "3. **Evaluation Variables (Step 7):**\n",
    "   - `model_path`: Provide the path to the directory where your fine-tuned BERT model is saved.\n",
    "   - `eval_df`: Load your evaluation data from a DataFrame or other data source and assign it to `eval_df`.\n",
    "   - `X_eval`: Input data for evaluation (text content). You need to provide the evaluation data, which should be a list of text samples.\n",
    "   - `y_eval`: Target labels for evaluation. You should provide the corresponding labels for your evaluation data.\n",
    "\n",
    "These variables require your input because they are specific to your project, data, and task. You should set them according to your dataset, the paths to your files, and your project's requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030fe8a5-c755-4375-a24d-3a8b642f984f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Step 1: Set Up Your SageMaker Environment\n",
    "\n",
    "Certainly! Documenting your code is essential for clarity and future reference. You can add comments and explanatory text using Markdown cells in your Jupyter Notebook. Here's an example of how you can document the code for step 1, along with explanations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc02c60d-37aa-4234-a1fe-e99d7e1596b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapy version: 2.10.1\n",
      "Pandas version: 2.1.0\n",
      "NumPy version: 1.25.2\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Set Up Your SageMaker Environment\n",
    "# -----------------------------------------\n",
    "\n",
    "# Import necessary libraries\n",
    "import scrapy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check the versions of libraries\n",
    "print(f'Scrapy version: {scrapy.__version__}')\n",
    "print(f'Pandas version: {pd.__version__}')\n",
    "print(f'NumPy version: {np.__version__}')\n",
    "\n",
    "# Explanation:\n",
    "# - In this step, we set up the Amazon SageMaker environment for our project.\n",
    "# - We create a SageMaker notebook instance and access Jupyter Notebook.\n",
    "# - Next, we start coding by importing essential libraries, including Scrapy for web scraping,\n",
    "#   pandas for data manipulation, numpy for numerical operations, and matplotlib for plotting.\n",
    "# - Finally, we print the versions of the imported libraries for reference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb48932-ca0b-440b-b187-44d673c770fa",
   "metadata": {},
   "source": [
    "In this example, the code is accompanied by comments that explain its purpose and the steps taken. Additionally, explanatory text in Markdown cells provides an overview of what's happening in the code cell.\n",
    "\n",
    "You can follow a similar approach throughout your Jupyter Notebook to ensure your code is well-documented, making it easier for you and others to understand and follow your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8a33f9-76e2-41ea-a16e-7b227cf5cef0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Step 2: Create a Scrapy Project and Spider for Web Scraping\n",
    "\n",
    "Certainly! In step 2, we'll create a Scrapy project and spider for web scraping. Make sure you have Scrapy installed in your SageMaker environment before proceeding. You can install it using !pip install scrapy if it's not already installed.\n",
    "\n",
    "Here's the code for creating a Scrapy project and spider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fe8562-bcda-414c-b96f-3df3c8dc2154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: Create a Scrapy Project and Spider for Web Scraping\n",
    "\n",
    "# Import the necessary Scrapy libraries\n",
    "import os\n",
    "import scrapy\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Define the Scrapy project name and spider name\n",
    "project_name = \"pythonorg_scrapy_project\"\n",
    "spider_name = \"pythonorg_spider\"\n",
    "\n",
    "# Create a Scrapy project\n",
    "if not os.path.exists(project_name):\n",
    "    !scrapy startproject {project_name}\n",
    "\n",
    "# Create a Scrapy spider\n",
    "os.chdir(project_name)\n",
    "if not os.path.exists(f'{project_name}/spiders/{spider_name}.py'):\n",
    "    with open(f'{project_name}/spiders/{spider_name}.py', 'w') as spider_file:\n",
    "        spider_file.write(f'''\n",
    "import scrapy\n",
    "\n",
    "class {spider_name.capitalize()}Spider(scrapy.Spider):\n",
    "    name = '{spider_name}'\n",
    "    start_urls = ['https://www.python.org/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extract all text from the current page and yield it\n",
    "        yield {\n",
    "            'text': response.css('::text').getall()\n",
    "        }\n",
    "\n",
    "        # Follow links to other pages on python.org\n",
    "        for next_page in response.css('a::attr(href)').getall():\n",
    "            yield response.follow(next_page, self.parse)\n",
    "''')\n",
    "\n",
    "# Explanation:\n",
    "# - In this modified step, we create a Scrapy project and spider to scrape all text from every page of python.org.\n",
    "# - We define the project name, spider name, and the starting URL as 'https://www.python.org/'.\n",
    "# - The `parse` method extracts all text from the current page and yields it in a dictionary.\n",
    "# - It then follows links to other pages on python.org and continues scraping text from those pages as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6384f541-ca16-4afd-8ebc-7bcc22b02d00",
   "metadata": {},
   "source": [
    "This code sets up a Scrapy project named \"pythonorg_scrapy_project\" and a spider named \"pythonorg_spider\" that starts with the URL \"https://www.python.org/\". You can customize the spider's logic within the generated spider file to perform the web scraping tasks you require."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa70502-bedd-41c2-a7cd-33665d6b621f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Step 3: Configure Spider Rules\n",
    "\n",
    "Thank you for your kind words! In step 3, we'll configure the spider rules for the Scrapy spider. This includes setting up rules for following links and defining how data is extracted from web pages.\n",
    "\n",
    "Here's the code for configuring the spider rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14e6f638-703a-4442-a837-549107f780a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3: Configure Spider Rules\n",
    "# ------------------------------\n",
    "\n",
    "# Import the necessary Scrapy libraries\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "\n",
    "# Define the spider rules\n",
    "class PythonOrgCrawler(CrawlSpider):\n",
    "    name = 'pythonorg'\n",
    "    allowed_domains = ['python.org']\n",
    "    start_urls = ['https://www.python.org/']\n",
    "\n",
    "    # Initialize a list to store text data\n",
    "    text_data = []\n",
    "\n",
    "    # Define rules for following links and extracting data\n",
    "    rules = (\n",
    "        # Rule to follow links within the same domain (python.org)\n",
    "        Rule(LinkExtractor(allow=('/')), callback='parse_item', follow=True),\n",
    "    )\n",
    "\n",
    "    # Define the parsing logic for extracting data from web pages\n",
    "    def parse_item(self, response):\n",
    "        # Extract and process data from the current page\n",
    "        text_content = response.css('::text').getall()\n",
    "        \n",
    "        # Remove any empty or whitespace-only strings from the text content\n",
    "        text_content = [text.strip() for text in text_content if text.strip()]\n",
    "        \n",
    "        # Join the extracted text content into a single string\n",
    "        text_content = ' '.join(text_content)\n",
    "        \n",
    "        # Store the extracted text content in a list (you can change the data storage method)\n",
    "        self.log(f'Extracted text from {response.url}')\n",
    "        self.text_data.append(text_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16867756-ccb7-4872-840a-e3826db859e3",
   "metadata": {},
   "source": [
    "In this code, we define the rules for the Scrapy spider to follow links and specify the callback method (parse_item) that is used to extract data from web pages. You can modify the rules and the parsing logic as needed for your web scraping project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514fc614-31b8-459d-8f77-3c22d8252c02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Step 4: Storing Data in CSV\n",
    "\n",
    "I'm glad to hear that you're impressed! In step 4, we'll implement code for storing the scraped data in CSV files. Here's the code for this step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a00f9b74-1db0-4692-b314-4faaf6cd27ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 4: Storing Data in CSV\n",
    "# ---------------------------\n",
    "\n",
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "from your_project_name.items import PythonorgItem  # Import the item class from your project\n",
    "\n",
    "# Create a DataFrame to store the scraped data\n",
    "data = []\n",
    "\n",
    "# Loop through the scraped items and append them to the data list\n",
    "for item in PythonorgItem:  # Use the appropriate item class from your project\n",
    "    data.append({\n",
    "        \"text_content\": item['text_content'],  # Adjust the field name based on your item structure\n",
    "        \"url\": item['url'],  # If you're storing URLs, adjust this field\n",
    "    })\n",
    "\n",
    "# Define the CSV filename\n",
    "csv_filename = \"pythonorg_data.csv\"\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(csv_filename, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Explanation:\n",
    "# - We import pandas to work with DataFrames.\n",
    "# - We import the item class (PythonorgItem) from your Scrapy project to access scraped data.\n",
    "# - We create an empty list 'data' to store the scraped data.\n",
    "# - We loop through the scraped items (PythonorgItem) and append relevant fields to the 'data' list.\n",
    "# - You should adjust the field names and item structure based on your Scrapy project.\n",
    "# - We define the CSV filename where the data will be saved.\n",
    "# - We create a pandas DataFrame from the 'data' list.\n",
    "# - Finally, we save the DataFrame to a CSV file with specified parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3026c5ea-2e5f-4bb9-813c-1e8d9b2eedf3",
   "metadata": {},
   "source": [
    "In this code, we create a sample DataFrame containing scraped data (you should replace this with your actual scraped data). The data is then saved to a CSV file named \"pythonorg_data.csv.\" You can adapt this code to store your scraped data in CSV files as needed for your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7effccf-2281-473d-88a0-662eb141ae3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Step 5: Data Processing\n",
    "\n",
    "In step 5, we'll implement code to process and preprocess the scraped data as necessary. Here's a basic example of how to perform data processing within your Jupyter Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62e1d7f3-8eaf-4317-91d4-f68d286b293c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Page 1</td>\n",
       "      <td>Content of Page 1</td>\n",
       "      <td>https://example.com/page1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Page 2</td>\n",
       "      <td>Content of Page 2</td>\n",
       "      <td>https://example.com/page2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    title            content                        url\n",
       "0  Page 1  Content of Page 1  https://example.com/page1\n",
       "1  Page 2  Content of Page 2  https://example.com/page2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Data Processing\n",
    "# -----------------------\n",
    "\n",
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Define a function to remove HTML tags from text\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Define the CSV filenames for input and output\n",
    "input_csv_filename = \"pythonorg_data.csv\"\n",
    "output_csv_filename = \"pythonorg_processed_data.csv\"\n",
    "\n",
    "try:\n",
    "    # Read the scraped data from the CSV file\n",
    "    df = pd.read_csv(input_csv_filename)\n",
    "\n",
    "    # Data Processing: Remove HTML tags from 'content' column\n",
    "    df['content'] = df['content'].apply(remove_html_tags)\n",
    "\n",
    "    # Additional data processing steps can be added here\n",
    "    # For example, you can perform text cleaning, tokenization, or feature engineering\n",
    "\n",
    "    # Display the first few rows of the processed DataFrame\n",
    "    print(\"Processed DataFrame:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Save the processed DataFrame to a new CSV file\n",
    "    df.to_csv(output_csv_filename, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Processed data saved to {output_csv_filename}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# Explanation:\n",
    "# - In this step, we perform data processing on the scraped data.\n",
    "# - We read the scraped data from the input CSV file into a pandas DataFrame.\n",
    "# - We define a function to remove HTML tags from the 'content' column.\n",
    "# - Additional data processing steps can be added as needed.\n",
    "# - The processed DataFrame is displayed for review.\n",
    "# - The processed data is saved to a new CSV file.\n",
    "# - Error handling is included to catch and display any exceptions that may occur during processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcba9623-b9ed-4c1d-b3bc-329fa50b2215",
   "metadata": {},
   "source": [
    "In this code:\n",
    "\n",
    "We read the scraped data from the CSV file into a pandas DataFrame.\n",
    "We provide an example of data processing by removing HTML tags from the 'content' column using BeautifulSoup. You can add more processing steps as required.\n",
    "The processed DataFrame is displayed to show the effect of the data processing.\n",
    "You can adapt and expand this code to perform specific data processing tasks based on the nature of your scraped data and the requirements of your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ba169-eed8-4fc9-88de-a6431d21e907",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 6: Fine-Tuning BERT\n",
    "\n",
    "I'm glad to hear that you're finding this information helpful! In step 6, we'll implement code for fine-tuning your BERT model using the scraped and preprocessed data. Below is a high-level example of how you can perform fine-tuning using the Hugging Face Transformers library:\n",
    "\n",
    "Please note that fine-tuning a BERT model typically requires a significant amount of computational resources and may take a considerable amount of time. Additionally, the example provided is a simplified illustration, and fine-tuning BERT effectively often involves tuning hyperparameters, managing checkpoints, and dealing with GPU resources, which are not covered in this simplified example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf14ef-9104-4c36-8273-8ee3e1a7ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Fine-Tuning BERT\n",
    "# ------------------------\n",
    "\n",
    "# Import the necessary libraries\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load your scraped and preprocessed data (assuming it's in a DataFrame 'df')\n",
    "# Replace 'text_content' and 'label' with your actual column names\n",
    "df = pd.read_csv(\"your_preprocessed_data.csv\")  # Replace with the path to your preprocessed data\n",
    "\n",
    "# Extract the text data and labels from the DataFrame\n",
    "X_train = df['text_content'].tolist()\n",
    "y_train = df['label'].tolist()\n",
    "\n",
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # Assuming binary classification\n",
    "\n",
    "# Tokenize and preprocess the training data\n",
    "inputs = tokenizer(X_train, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "labels = torch.tensor(y_train)\n",
    "\n",
    "# Set up optimizer and training parameters\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 5  # Adjust the number of epochs based on your dataset and task\n",
    "\n",
    "try:\n",
    "    # Fine-tune the BERT model\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print or log loss and other metrics (if needed) to monitor progress\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained(\"fine_tuned_bert\")\n",
    "    print(\"Fine-tuned model saved successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede9316-55d8-45a8-a3e5-659ee305174d",
   "metadata": {},
   "source": [
    "Please keep in mind that fine-tuning BERT effectively often requires a deep understanding of natural language processing tasks, model hyperparameter tuning, and substantial computational resources. The example provided is a simplified illustration, and fine-tuning should be performed carefully, possibly using larger datasets and more complex architectures depending on your specific NLP task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d9e8cf-8dc8-450b-9698-6d3a038b1072",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 7: Evaluation and Testing\n",
    "\n",
    "In step 7, we'll focus on evaluating the fine-tuned BERT model's performance and testing it on sample Python-related tasks. Here's an example of how you can perform evaluation and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f3f1b-86e8-480e-adf2-71c881cf18ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Evaluation and Testing\n",
    "# ------------------------------\n",
    "\n",
    "# Import the necessary libraries for evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load the fine-tuned BERT model\n",
    "model_path = \"fine_tuned_bert\"  # Replace with the path to your fine-tuned model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Load your evaluation data from a DataFrame (eval_df)\n",
    "# Replace 'text_content' and 'label' with your actual column names\n",
    "eval_df = pd.read_csv(\"your_evaluation_data.csv\")  # Replace with the path to your evaluation data\n",
    "\n",
    "# Extract the text data and labels from the DataFrame\n",
    "X_eval = eval_df['text_content'].tolist()\n",
    "y_eval = eval_df['label'].tolist()\n",
    "\n",
    "# Tokenize and preprocess the evaluation data\n",
    "inputs = tokenizer(X_eval, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "labels = torch.tensor(y_eval)\n",
    "\n",
    "# Evaluate the fine-tuned model on the evaluation data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Predictions\n",
    "predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(labels, predictions)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231afd2-ecb1-4847-afe5-5626f27ac6b4",
   "metadata": {},
   "source": [
    "This code provides a basic example of how to evaluate your fine-tuned BERT model's performance using accuracy and a classification report. Depending on your project's goals and tasks, you may need to implement more specialized evaluation metrics and explore the model's performance in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cec9c5e-1556-4b6a-8786-9f286906e183",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Step 8\n",
    "\n",
    "In step 8, we'll discuss how you can automate the web scraping, fine-tuning, and other tasks in your project using a script. Automating these tasks allows you to execute them without manual intervention. Below is a high-level overview of how you can achieve automation:\n",
    "\n",
    "Script Organization: Create a Python script (e.g., a .py file) that contains the code for all the steps in your project, from web scraping to fine-tuning BERT.\n",
    "Parameterization: Make the script configurable by allowing input parameters or configuration files. This way, you can easily adjust settings for different runs of your project.\n",
    "Automate Data Collection: Implement a function or a section of the script to automate web scraping using Scrapy. You can use scheduling tools like cron jobs to run the script at specified intervals.\n",
    "Data Preprocessing: Include data preprocessing steps in the script to clean and structure the scraped data.\n",
    "BERT Fine-Tuning: Automate the fine-tuning of your BERT model using the script. Ensure that it loads the scraped data and fine-tunes the model without manual intervention.\n",
    "Evaluation and Testing: Automate the evaluation of the fine-tuned model on a validation or test dataset. Store and log evaluation results.\n",
    "Logging and Reporting: Implement logging to record the progress and outcomes of each run. This helps in tracking errors and monitoring the performance of your automated tasks.\n",
    "Error Handling: Include error-handling mechanisms in the script to handle exceptions gracefully and send notifications if errors occur.\n",
    "Scheduling: Set up a scheduling mechanism (e.g., cron jobs on Linux or Task Scheduler on Windows) to run the script at specified intervals or times. This allows for regular updates and retraining of the model.\n",
    "Monitoring: Consider implementing monitoring and alerting systems to notify you of script failures or issues with web scraping.\n",
    "Deployment: If needed, deploy your automated script on a server or cloud instance to ensure continuous execution.\n",
    "Documentation: Document the script thoroughly, including how to configure it, set up scheduling, and interpret the results.\n",
    "Here's a simplified example of what the script organization might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f44283d-24b6-4c2d-822c-cec8e4fe7a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8\n",
    "# ------\n",
    "\n",
    "# Import necessary libraries\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def main():\n",
    "    # Step 1: Web scraping using Scrapy\n",
    "    logging.info(\"Starting web scraping...\")\n",
    "    # Implement your web scraping logic here\n",
    "\n",
    "    # Step 2: Data preprocessing\n",
    "    logging.info(\"Performing data preprocessing...\")\n",
    "    # Implement data preprocessing steps\n",
    "\n",
    "    # Step 3: Fine-tuning BERT model\n",
    "    logging.info(\"Fine-tuning BERT model...\")\n",
    "    # Implement BERT fine-tuning logic\n",
    "\n",
    "    # Step 4: Evaluation and testing\n",
    "    logging.info(\"Evaluating the fine-tuned model...\")\n",
    "    # Implement model evaluation and testing\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Automate web scraping and BERT fine-tuning.\")\n",
    "    # Define command-line arguments or configuration parameters here\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Call the main function to start the automation\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459a29df-a958-4192-ae10-86c854da4d2f",
   "metadata": {},
   "source": [
    "This is a simplified template for your automation script. You would customize it with your specific logic, data handling, and error handling mechanisms. Once the script is set up, you can schedule it to run periodically, ensuring that your web scraping and fine-tuning tasks are automated without manual intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdcbb78-8a6f-44c2-aee8-21d1a90bea0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Step 9: Scrape the Entire Site\n",
    "\n",
    "Step 9 involves scraping the entire site without limitations on the number of pages. This requires setting up your web scraping process to crawl through all available pages on the website. Here's how you can approach it using Scrapy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d64f2-4e4d-4c3e-be91-246e3e02cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Scrape the Entire Site\n",
    "# ------------------------------\n",
    "\n",
    "# Import the necessary Scrapy libraries\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Define the Scrapy spider to scrape the entire site\n",
    "class PythonOrgCrawler(scrapy.Spider):\n",
    "    name = 'pythonorg'\n",
    "    allowed_domains = ['python.org']\n",
    "    start_urls = ['https://www.python.org/']\n",
    "\n",
    "    # Define rules for following links and extracting data\n",
    "    def parse(self, response):\n",
    "        # Extract and process data from the current page\n",
    "        # Here, you can customize data extraction logic based on your needs\n",
    "\n",
    "        # Follow all links on the current page to continue scraping\n",
    "        for link in response.css('a::attr(href)').extract():\n",
    "            yield response.follow(link, callback=self.parse)\n",
    "\n",
    "# Configure and start the Scrapy crawler process\n",
    "process = CrawlerProcess()\n",
    "process.crawl(PythonOrgCrawler)\n",
    "process.start()\n",
    "\n",
    "# Explanation:\n",
    "# - In this step, we create a Scrapy spider that starts at the website's root URL ('https://www.python.org/') and follows links to scrape the entire site.\n",
    "# - The parse method extracts and processes data from the current page.\n",
    "# - By using a loop and response.follow, the spider follows all links on the current page to crawl through the entire site.\n",
    "# - The Scrapy crawler process is configured and started to execute the spider.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef4476c-fb2d-4310-8ed4-88fc45fbbdd7",
   "metadata": {},
   "source": [
    "This code configures a Scrapy spider to start at the root URL of the website and recursively follow links to scrape all pages on the site. Be cautious when scraping large websites, as it can generate a substantial amount of data and may be subject to rate-limiting or other restrictions set by the website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb58a56-8511-4e3b-af60-477fd8c11973",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Step 10\n",
    "\n",
    "Great, we've reached the final step! In step 10, we'll discuss how to efficiently store the scraped data. You mentioned earlier that you'd like to use CSV for storage. Here's how you can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b9cad9-bde6-4cbc-b50e-035d9ff74def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Store Scraped Data Efficiently\n",
    "# ---------------------------------------\n",
    "\n",
    "# Import the necessary libraries\n",
    "import scrapy\n",
    "import pandas as pd\n",
    "\n",
    "# Define a Scrapy pipeline for efficient data storage\n",
    "class CsvExportPipeline:\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        # Append scraped data to the data list\n",
    "        self.data.append(item)\n",
    "        return item\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        # Convert the data list to a DataFrame\n",
    "        df = pd.DataFrame(self.data)\n",
    "\n",
    "        # Define the CSV filename\n",
    "        csv_filename = \"pythonorg_data.csv\"\n",
    "\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df.to_csv(csv_filename, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Configure Scrapy settings to use the CsvExportPipeline\n",
    "ITEM_PIPELINES = {'your_project_name.pipelines.CsvExportPipeline': 300}\n",
    "\n",
    "# Explanation:\n",
    "# - In this step, we create a Scrapy pipeline for efficient data storage in CSV format.\n",
    "# - The CsvExportPipeline class accumulates scraped data in a list during the scraping process.\n",
    "# - The process_item method appends scraped items (data) to the list.\n",
    "# - When the spider is closed (the close_spider method is called), the data list is converted to a pandas DataFrame.\n",
    "# - We define the CSV filename where the data will be saved (replace \"pythonorg_data.csv\" with your desired filename).\n",
    "# - Finally, the DataFrame is saved to a CSV file, resulting in efficient storage of your scraped data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9100a3d-b782-41e4-a829-61efa710199a",
   "metadata": {},
   "source": [
    "To implement this pipeline, you'l need to make sure it's part of your Scrapy project's configuration. You can specify the pipeline in your Scrapy settings (`settings.py`) by adding the `ITEM_PIPELINES` configuration as shown above. Replace `'your_project_name'` with the actual name of your Scrapy project.\n",
    "\n",
    "This pipeline efficiently stores the scraped data as it accumulates during the scraping process and saves it to a CSV file when the spider is closed, ensuring that you have a well-structured dataset for further analysis or fine-tuning your BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d794daa0-7665-4188-b3ef-cffee9d1d3ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "plyxal-ai (arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1)",
   "language": "python",
   "name": "plyxal-ai__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
